{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python\n",
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# # -----------------------------------------------\n",
    "\n",
    "# ### Import Libraries\n",
    "# import os\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "# from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "# from torchvision.transforms import functional as F\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# #from utils.helper import fn_plot_confusion_matrix, fn_plot_tf_hist\n",
    "\n",
    "# # ----------------------------------------------------- \n",
    "\n",
    "# # Some basic parameters\n",
    "\n",
    "# inpDir = '../../../input' # location where input data is stored\n",
    "# outDir = '../output' # location to store outputs\n",
    "# subDir = 'basic_operations' # location of the images\n",
    "# modelDir = '../models'\n",
    "# altName = 'rcnn'\n",
    "\n",
    "\n",
    "# RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "# EPOCHS = 100 # number of cycles to run\n",
    "# THRESHOLD = 0.8\n",
    "\n",
    "\n",
    "# # Set parameters for decoration of plots\n",
    "# params = {'legend.fontsize' : 'large',\n",
    "#           'figure.figsize'  : (15,12),\n",
    "#           'axes.labelsize'  : 'x-large',\n",
    "#           'axes.titlesize'  :'x-large',\n",
    "#           'xtick.labelsize' :'large',\n",
    "#           'ytick.labelsize' :'large',\n",
    "#          }\n",
    "\n",
    "# CMAP = plt.cm.brg\n",
    "\n",
    "# plt.rcParams.update(params) # update rcParams\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# # print ('Is CUDA available: ', torch.cuda.is_available())\n",
    "# # print ('CUDA version: ', torch.version.cuda )\n",
    "# # print ('Current Device ID: ', torch.cuda.current_device())\n",
    "# # print ('Name of the CUDA device: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# # Get cpu or gpu device for training.\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "#     '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "#     'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "#     'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "#     'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "#     'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "#     'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "#     'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "#     'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "#     'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "#     'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "#     'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "#     'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "# ]\n",
    "\n",
    "# # Create different colors for each class.\n",
    "# COLORS = np.random.uniform(0, 255, size=(len(COCO_INSTANCE_CATEGORY_NAMES), 3))\n",
    "# # Define the torchvision image transforms.\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# def predict(image, model, device, detection_threshold = 0.5, coco_names=COCO_INSTANCE_CATEGORY_NAMES):\n",
    "#     \"\"\"\n",
    "#     Predict the output of an image after forward pass through\n",
    "#     the model and return the bounding boxes, class names, and\n",
    "#     class labels.\n",
    "#     \"\"\"\n",
    "#     # Transform the image to tensor.\n",
    "#     image = transform(image).to(device)\n",
    "#     # Add a batch dimension.\n",
    "#     image = image.unsqueeze(0)\n",
    "#     # Get the predictions on the image.\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(image)\n",
    "#     # Get score for all the predicted objects.\n",
    "#     pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "#     # Get all the predicted bounding boxes.\n",
    "#     pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "#     # Get boxes above the threshold score.\n",
    "#     boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "#     labels = outputs[0]['labels'][:len(boxes)]\n",
    "#     # Get all the predicted class names.\n",
    "#     pred_classes = [coco_names[i] for i in labels.cpu().numpy()]\n",
    "#     return boxes, pred_classes, labels\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# def draw_boxes(boxes, classes, labels, image):\n",
    "#     '''`\n",
    "#     Draws bounding boxes with labels on an image to represent detected objects.\n",
    "\n",
    "#     Args:\n",
    "#         boxes (list): A list of bounding box coordinates for each detected object,\n",
    "#                     where each box is represented by [x_min, y_min, x_max, y_max].\n",
    "#         classes (list): A list of class names corresponding to each bounding box.\n",
    "#         labels (list): A list of label indices for each detected object; these are\n",
    "#                     used to pick specific colors for each class.\n",
    "#         image (numpy array): The image on which to draw the bounding boxes.\n",
    "\n",
    "#     Returns:\n",
    "#         image (numpy array): The modified image with bounding boxes and labels drawn on it.\n",
    "#     '''\n",
    "\n",
    "#     # Determine line width for the bounding boxes based on image dimensions.\n",
    "#     # This sets the line width to a fraction of the sum of image height and width,\n",
    "#     # with a minimum width of 2 to ensure visibility.\n",
    "#     lw = max(round(sum(image.shape) / 2 * 0.001), 2)\n",
    "\n",
    "#     # Determine font thickness, set to one less than line width but with a minimum of 1.\n",
    "#     tf = max(lw - 1, 2)\n",
    "\n",
    "#     # Loop through each bounding box to draw it on the image.\n",
    "#     for i, box in enumerate(boxes):\n",
    "#         # Select a color for the bounding box from the COLORS array, using the label index.\n",
    "#         # COLORS is a predefined array where each index corresponds to a specific color.\n",
    "#         color = COLORS[labels[i]]\n",
    "\n",
    "#         # Draw the bounding box as a rectangle on the image.\n",
    "\n",
    "#         cv2.rectangle(\n",
    "#             img=image,\n",
    "#             pt1=(int(box[0]), int(box[1])), # top-left corner of the box.\n",
    "#             pt2=(int(box[2]), int(box[3])), # bottom-right corner of the box.\n",
    "#             color=color[::-1],              # Reversed to BGR for OpenCV\n",
    "#             thickness=lw                    # set to lw to maintain consistency with text.\n",
    "#         )\n",
    "\n",
    "#         # Add text to label the bounding box with the class name.\n",
    "#         cv2.putText(\n",
    "#             img=image,\n",
    "#             text=classes[i],\n",
    "#             org=(int(box[0]), int(box[1] - 5)),  # 5 pixels above the top-left corner\n",
    "#             fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#             fontScale=lw / 2,                    # proportional to line width for a balanced appearance.\n",
    "#             color=color[::-1],                   # Reversed to BGR for OpenCV\n",
    "#             thickness=tf,                        # set to tf for good legibility.\n",
    "#             lineType=cv2.LINE_AA                 # anti-aliased text for smoothness.\n",
    "#         )\n",
    "\n",
    "#     # Return the modified image with all bounding boxes and labels.\n",
    "#     return image\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# # Load a pre-trained Faster R-CNN model\n",
    "# def get_model(device='cuda'):\n",
    "\n",
    "#     # Load the model.\n",
    "#     model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "\n",
    "#     # Load the model onto the computation device.\n",
    "#     model = model.eval().to(device)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# model = get_model(device)\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# # Load the input image and apply transformations\n",
    "# def load_image(image_path):\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "#     # Create a BGR copy of the image for annotation.\n",
    "#     image_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "#     return image, image_bgr\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# imgName = 'traffic.jpg' # 'IMG_1295.JPG'\n",
    "# # Load an example image\n",
    "# image_path = os.path.join(inpDir, subDir, imgName)  # specify your image path\n",
    "# image, image_bgr = load_image(imgName)\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# plt.imshow(image_bgr)\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# # Detect outputs.\n",
    "# with torch.no_grad():\n",
    "#     boxes, classes, labels = predict(image, model, device, detection_threshold= THRESHOLD)\n",
    "# # Draw bounding boxes.\n",
    "# image = draw_boxes(boxes, classes, labels, image_bgr)\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# type(image_bgr)\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# rgbIm = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(rgbIm);\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# vidName = 'VID_20240320164919_F (online-video-cutter.com).mp4' #'VID_20240320164819_F.MOV' # 'VID_20240320164919_F.MOV'\n",
    "# # vidFilePath = os.path.join(inpDir, subDir, vidName)\n",
    "\n",
    "# cap = cv2.VideoCapture(vidName)\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# if (cap.isOpened() == False):\n",
    "#     print('Error while trying to read video. Please check path again')\n",
    "# # Get the frame width and height.\n",
    "# frame_width = int(cap.get(3))\n",
    "# frame_height = int(cap.get(4))\n",
    "\n",
    "# # for saving the file\n",
    "# save_name = f\"{vidName[-1].split('.')[0]}_t{''.join(str(THRESHOLD).split('.'))}_{altName}\"\n",
    "# # Define codec and create VideoWriter object .\n",
    "# out = cv2.VideoWriter(os.path.join(outDir, f\"{save_name}.mp4\"),\n",
    "#                       cv2.VideoWriter_fourcc(*'mp4v'), 30,\n",
    "#                       (frame_width, frame_height))\n",
    "# frame_count = 0 # To count total frames.\n",
    "# total_fps = 0 # To get the final frames per second.\n",
    "\n",
    "# # -----------------------------------------------------\n",
    "\n",
    "# # Get the start time for the entire process.\n",
    "# process_start_time = time.time()\n",
    "\n",
    "# # Read until end of video.\n",
    "# while(cap.isOpened):\n",
    "#     # Capture each frame of the video.\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret:\n",
    "#         frame_copy = frame.copy()\n",
    "#         frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "#         # Get the start time.\n",
    "#         start_time = time.time()\n",
    "#         with torch.no_grad():\n",
    "#             # Get predictions for the current frame.\n",
    "#             boxes, classes, labels = predict(frame, model, device, THRESHOLD)\n",
    "\n",
    "#         # Draw boxes and show current frame on screen.\n",
    "#         image = draw_boxes(boxes, classes, labels, frame)\n",
    "#         # Get the end time.\n",
    "#         end_time = time.time()\n",
    "#         # Get the fps.\n",
    "#         fps = 1 / (end_time - start_time)\n",
    "#         # Add fps to total fps.\n",
    "#         total_fps += fps\n",
    "#         # Increment frame count.\n",
    "#         frame_count += 1\n",
    "#         # Write the FPS on the current frame.\n",
    "#         cv2.putText(\n",
    "#             img=image,\n",
    "#             text=f\"{fps:.3f} FPS\",\n",
    "#             org=(15, 30),\n",
    "#             fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#             fontScale=1,\n",
    "#             color=(0, 255, 0),\n",
    "#             thickness=2,\n",
    "#             lineType=cv2.LINE_AA\n",
    "#         )\n",
    "#         # Convert from BGR to RGB color format.\n",
    "#         cv2.imshow('image', image)\n",
    "#         out.write(image)\n",
    "#         # Press `q` to exit.\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     else:\n",
    "#         break\n",
    "# # Release VideoCapture().\n",
    "# cap.release()\n",
    "# # Close all frames and video windows.\n",
    "# cv2.destroyAllWindows()\n",
    "# # Calculate and print the average FPS.\n",
    "# avg_fps = total_fps / frame_count\n",
    "# print(f\"Average FPS: {avg_fps:.3f}\")\n",
    "\n",
    "# # Get the end time for the entire process.\n",
    "# process_end_time = time.time()\n",
    "# # Calculate the total time taken for the entire process.\n",
    "# total_process_time = process_end_time - process_start_time\n",
    "# print(f\"Total time taken: {total_process_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# OpenCV CUDA acceleration (if available)\n",
    "use_cuda_opencv = cv2.cuda.getCudaEnabledDeviceCount() > 0\n",
    "\n",
    "# Function to preprocess frames with CUDA acceleration\n",
    "def preprocess_frame(frame, target_size=(800, 800)):\n",
    "    if use_cuda_opencv:\n",
    "        # Convert frame to GPU\n",
    "        gpu_frame = cv2.cuda_GpuMat()\n",
    "        gpu_frame.upload(frame)\n",
    "\n",
    "        # Resize using OpenCV CUDA\n",
    "        gpu_frame = cv2.cuda.resize(gpu_frame, target_size)\n",
    "\n",
    "        # Download back to CPU as numpy array\n",
    "        frame = gpu_frame.download()\n",
    "\n",
    "    # Convert frame to PyTorch tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    frame_tensor = transform(frame).to(device)\n",
    "    return frame_tensor\n",
    "\n",
    "# Function to process batch of frames\n",
    "def process_batch(frames_batch, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames_batch)\n",
    "    return outputs\n",
    "\n",
    "# Function to draw detections on the frame\n",
    "def draw_detections(frame, detections, threshold=0.5):\n",
    "    for i in range(len(detections[\"boxes\"])):\n",
    "        score = detections[\"scores\"][i].item()\n",
    "        if score >= threshold:\n",
    "            x1, y1, x2, y2 = map(int, detections[\"boxes\"][i].tolist())\n",
    "            label = f\"Object {detections['labels'][i].item()} ({score:.2f})\"\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "# Main function to process video with FPS calculation\n",
    "def process_video(video_path, model, batch_size=4):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    frames_batch = []\n",
    "    original_frames = []  # Store original frames for drawing later\n",
    "    frame_count = 0\n",
    "    total_time = 0.0  # To calculate average FPS\n",
    "\n",
    "    while True:\n",
    "        start_time = time.time()  # Start time for FPS calculation\n",
    "        \n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break  # End of video\n",
    "\n",
    "        original_frames.append(frame)\n",
    "        frame_tensor = preprocess_frame(frame)\n",
    "        frames_batch.append(frame_tensor)\n",
    "\n",
    "        if len(frames_batch) == batch_size:\n",
    "            # Stack batch and move to GPU\n",
    "            frames_batch_tensor = torch.stack(frames_batch).to(device)\n",
    "\n",
    "            # Run inference\n",
    "            outputs = process_batch(frames_batch_tensor, model)\n",
    "\n",
    "            # Process each frame output\n",
    "            for i in range(len(outputs)):\n",
    "                original_frames[i] = draw_detections(original_frames[i], outputs[i])\n",
    "\n",
    "                # Calculate FPS\n",
    "                end_time = time.time()\n",
    "                fps = 1 / (end_time - start_time)\n",
    "                total_time += (end_time - start_time)\n",
    "                frame_count += 1\n",
    "\n",
    "                # Display FPS on frame\n",
    "                cv2.putText(original_frames[i], f\"FPS: {fps:.2f}\", (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "                # Show frame\n",
    "                cv2.imshow(\"Real-Time Object Detection\", original_frames[i])\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            # Clear batch lists\n",
    "            frames_batch = []\n",
    "            original_frames = []\n",
    "\n",
    "    # Calculate and print average FPS\n",
    "    avg_fps = frame_count / total_time if total_time > 0 else 0\n",
    "    print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "\n",
    "    # Release resources\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the pipeline\n",
    "video_path = \"VID_20240320164919_F (online-video-cutter.com).mp4\"\n",
    "process_video(video_path, model, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
